{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayyubi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-04 16:46:28,155 - INFO - Processing Word document: ../../../data/convictions/transcripts/iterative\\(C) Det. Martin Venezia Testimony - Trial One.docx\n",
      "2023-08-04 16:46:28,200 - INFO - Text loaded from Word document: ../../../data/convictions/transcripts/iterative\\(C) Det. Martin Venezia Testimony - Trial One.docx\n",
      "2023-08-04 16:46:30,775 - INFO - Loading faiss with AVX2 support.\n",
      "2023-08-04 16:46:30,776 - INFO - Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "2023-08-04 16:46:30,777 - INFO - Loading faiss.\n",
      "2023-08-04 16:46:30,813 - INFO - Successfully loaded faiss.\n",
      "2023-08-04 16:46:30,854 - INFO - Performing query...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Document' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 248\u001b[0m\n\u001b[0;32m    245\u001b[0m     process_query(embeddings)\n\u001b[0;32m    247\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 248\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[1], line 245\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[0;32m    244\u001b[0m     embeddings \u001b[39m=\u001b[39m generate_hypothetical_embeddings()\n\u001b[1;32m--> 245\u001b[0m     process_query(embeddings)\n",
      "Cell \u001b[1;32mIn[1], line 227\u001b[0m, in \u001b[0;36mprocess_query\u001b[1;34m(embeddings)\u001b[0m\n\u001b[0;32m    225\u001b[0m db \u001b[39m=\u001b[39m preprocess_single_document(file_path, embeddings)\n\u001b[0;32m    226\u001b[0m \u001b[39mfor\u001b[39;00m query \u001b[39min\u001b[39;00m QUERIES:\n\u001b[1;32m--> 227\u001b[0m     officer_data, _ \u001b[39m=\u001b[39m get_response_from_query(db, query, TEMPERATURE, k)\n\u001b[0;32m    228\u001b[0m     \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m officer_data:\n\u001b[0;32m    229\u001b[0m         item[\u001b[39m\"\u001b[39m\u001b[39mQuery\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m query\n",
      "Cell \u001b[1;32mIn[1], line 140\u001b[0m, in \u001b[0;36mget_response_from_query\u001b[1;34m(db, query, temperature, k)\u001b[0m\n\u001b[0;32m    138\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mPerforming query...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    139\u001b[0m doc_list \u001b[39m=\u001b[39m db\u001b[39m.\u001b[39msimilarity_search(query, k\u001b[39m=\u001b[39mk)\n\u001b[1;32m--> 140\u001b[0m docs \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39;49m(doc_list, key\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m x: x[\u001b[39m1\u001b[39;49m], reverse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    142\u001b[0m third \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(docs) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m    144\u001b[0m highest_third \u001b[39m=\u001b[39m docs[:third]\n",
      "Cell \u001b[1;32mIn[1], line 140\u001b[0m, in \u001b[0;36mget_response_from_query.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    138\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mPerforming query...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    139\u001b[0m doc_list \u001b[39m=\u001b[39m db\u001b[39m.\u001b[39msimilarity_search(query, k\u001b[39m=\u001b[39mk)\n\u001b[1;32m--> 140\u001b[0m docs \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(doc_list, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39;49m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    142\u001b[0m third \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(docs) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m    144\u001b[0m highest_third \u001b[39m=\u001b[39m docs[:third]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Document' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain, HypotheticalDocumentEmbedder\n",
    "import re\n",
    "import pandas as pd\n",
    "import logging\n",
    "from summarizer import Summarizer\n",
    "import pprint\n",
    "\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(message)s\", level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "query_memory = []\n",
    "\n",
    "\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 250\n",
    "TEMPERATURE = 0\n",
    "k = 10\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE_MODEL = PromptTemplate(\n",
    "    input_variables=[\"question\", \"docs\"],\n",
    "    template=\"\"\"\n",
    "    As an AI assistant, my role is to meticulously analyze court transcripts and extract information about law enforcement personnel.\n",
    "    The names of law enforcement personnel will be prefixed by one of the following titles: officer, detective, deputy, lieutenant, \n",
    "    sergeant, captain, officer, coroner, investigator, criminalist, patrolman, or technician.\n",
    "\n",
    "    Query: {question}\n",
    "\n",
    "    Transcripts: {docs}\n",
    "\n",
    "    The response will contain:\n",
    "\n",
    "    1) The name of a officer, detective, deputy, lieutenant, \n",
    "       sergeant, captain, officer, coroner, investigator, criminalist, patrolman, or technician - \n",
    "       if an individual's name is not associated with one of these titles they do not work in law enforcement.\n",
    "       Please prefix the name with \"Officer Name: \". \n",
    "       For example, \"Officer Name: John Smith\".\n",
    "\n",
    "    2) If available, provide an in-depth description of the context of their mention. \n",
    "       If the context induces ambiguity regarding the individual's employment in law enforcement, \n",
    "       remove the individual.\n",
    "       Please prefix this information with \"Officer Context: \". \n",
    "\n",
    "    Continue this pattern of identifying persons, until all law enforcement personnel are identified.  \n",
    "\n",
    "    Additional guidelines for the AI assistant:\n",
    "    - Titles may be abbreviated to the following Sgt., Cpl, Cpt, Det., Ofc., Lt., P.O. and P/O\n",
    "    - Titles \"Technician\" and \"Tech\" might be used interchangeably.\n",
    "    - Derive responses from factual information found within the police reports.\n",
    "    - If the context of an identified person's mention is not clear in the report, provide their name and note that the context is not specified.\n",
    "    - Do not extract information about victims and witnesses\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE_HYDE = PromptTemplate(input_variables=[\"question\"], template=\"\"\"\n",
    "    You're an AI assistant specializing in criminal justice research. \n",
    "    Your main focus is on identifying the names and providing detailed context of mention for each law enforcement personnel. \n",
    "    This includes police officers, detectives, deupties, lieutenants, sergeants, captains, technicians, coroners, investigators, patrolman, and criminalists, \n",
    "    as described in court transcripts.\n",
    "    Be aware that the titles \"Detective\" and \"Officer\" might be used interchangeably.\n",
    "    Be aware that the titles \"Technician\" and \"Tech\" might be used interchangeably.\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Roles and Responses:\"\"\")\n",
    "\n",
    "\n",
    "def clean_name(officer_name):\n",
    "    return re.sub(\n",
    "        r\"(Detective|Officer|Deputy|Captain|[CcPpLl]|Sergeant|Lieutenant|Techn?i?c?i?a?n?|Investigator)\\.?\\s+\",\n",
    "        \"\",\n",
    "        officer_name,\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_officer_data(formatted_response):\n",
    "    officer_data = []\n",
    "    response_lines = formatted_response.split(\"\\n\")\n",
    "\n",
    "    for line in response_lines:\n",
    "        if line.startswith(\"Officer Name\"):\n",
    "            officer_name = line.split(\":\", 1)[1].strip()\n",
    "            officer_title = re.search(\n",
    "                r\"(Detective|Officer|Deputy|Captain|[CcPpLl]|Sergeant|Lieutenant|Techn?i?c?i?a?n?|Investigator)\\.?\",\n",
    "                officer_name,\n",
    "            )\n",
    "            if officer_title:\n",
    "                officer_title = officer_title.group()\n",
    "            else:\n",
    "                officer_title = \"\"\n",
    "            officer_name = clean_name(officer_name)\n",
    "        elif line.startswith(\"Officer Context\"):\n",
    "            split_line = line.split(\":\", 1)\n",
    "            if len(split_line) > 1:\n",
    "                officer_context = split_line[1].strip()\n",
    "            else:\n",
    "                officer_context = \"\"  \n",
    "            officer_data.append(\n",
    "                {\n",
    "                    \"Officer Name\": officer_name,\n",
    "                    \"Officer Context\": officer_context,\n",
    "                    \"Officer Title\": officer_title,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return officer_data\n",
    "\n",
    "\n",
    "def generate_hypothetical_embeddings():\n",
    "    llm = OpenAI()\n",
    "    prompt = PROMPT_TEMPLATE_HYDE\n",
    "\n",
    "    llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    base_embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    embeddings = HypotheticalDocumentEmbedder(\n",
    "        llm_chain=llm_chain, base_embeddings=base_embeddings\n",
    "    )\n",
    "    return base_embeddings\n",
    "\n",
    "\n",
    "def get_response_from_query(db, query, temperature, k):\n",
    "    logger.info(\"Performing query...\")\n",
    "    doc_list = db.similarity_search_with_score(query, k=k)\n",
    "\n",
    "    docs = sorted(doc_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    third = len(docs) // 3\n",
    "\n",
    "    highest_third = docs[:third]\n",
    "    middle_third = docs[third:2*third]\n",
    "    lowest_third = docs[2*third:]\n",
    "\n",
    "    highest_third = sorted(highest_third, key=lambda x: x[1], reverse=True)\n",
    "    middle_third = sorted(middle_third, key=lambda x: x[1], reverse=True)\n",
    "    lowest_third = sorted(lowest_third, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    docs = highest_third + lowest_third + middle_third\n",
    "\n",
    "    docs_page_content = \" \".join([d[0].page_content for d in docs])\n",
    "\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4\", verbose=True)\n",
    "\n",
    "    prompt = PROMPT_TEMPLATE_MODEL\n",
    "\n",
    "    chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n",
    "    response = chain.run(question=query, docs=docs_page_content, temperature=temperature)\n",
    "\n",
    "    formatted_response = \"\"\n",
    "    officers = response.split(\"Officer Name:\")\n",
    "    for i, officer in enumerate(officers):\n",
    "        if officer.strip() != \"\":\n",
    "            formatted_response += f\"Officer Name {i}:{officer.replace('Officer Context:', 'Officer Context ' + str(i) + ':')}\\n\\n\"\n",
    "\n",
    "    officer_data = extract_officer_data(formatted_response)\n",
    "    return officer_data,\n",
    "\n",
    "\n",
    "\n",
    "# QUERIES = [\n",
    "    # \"Identify individuals, by name, with the specific titles of officers, sergeants, lieutenants, captains, detectives, homicide officers, and crime lab personnel in the transcript. Specifically, provide the context of their mention related to key events in the case, if available.\",\n",
    "#     \"List individuals, by name, directly titled as officers, sergeants, lieutenants, captains, detectives, homicide units, and crime lab personnel mentioned in the transcript. Provide the context of their mention in terms of any significant decisions they made or actions they took.\",\n",
    "#     \"Locate individuals, by name, directly referred to as officers, sergeants, lieutenants, captains, detectives, homicide units, and crime lab personnel in the transcript. Explain the context of their mention in relation to their interactions with other individuals in the case.\",\n",
    "    # \"Highlight individuals, by name, directly titled as officers, sergeants, lieutenants, captains, detectives, homicide units, and crime lab personnel in the transcript. Describe the context of their mention, specifically noting any roles or responsibilities they held in the case.\",\n",
    "    # \"Outline individuals, by name, directly identified as officers, sergeants, lieutenants, captains, detectives, homicide units, and crime lab personnel in the transcript. Specify the context of their mention in terms of any noteworthy outcomes or results they achieved.\",\n",
    "    # \"Pinpoint individuals, by name, directly labeled as officers, sergeants, lieutenants, captains, detectives, homicide units, and crime lab personnel in the transcript. Provide the context of their mention, particularly emphasizing any significant incidents or episodes they were involved in.\",\n",
    "# ]\n",
    "\n",
    "\n",
    "QUERIES = [\n",
    "    \"Identify individuals, by name, with the specific titles of officers, sergeants, lieutenants, captains, detectives, homicide officers, and crime lab personnel in the transcript. Specifically, provide the context of their mention related to key events in the case, if available.\",\n",
    "]\n",
    "\n",
    "\n",
    "def summarize_context(context):\n",
    "    model = Summarizer()\n",
    "    result = model(context, min_length=60)\n",
    "    summary = \"\".join(result)\n",
    "    return summary\n",
    "\n",
    "\n",
    "def preprocess_single_document(file_path, embeddings):\n",
    "    logger.info(f\"Processing Word document: {file_path}\")\n",
    "\n",
    "    loader = Docx2txtLoader(file_path)\n",
    "    text = loader.load()\n",
    "    logger.info(f\"Text loaded from Word document: {file_path}\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "    docs = text_splitter.split_documents(text)\n",
    "\n",
    "    db = FAISS.from_documents(docs, embeddings)\n",
    "    return db\n",
    "\n",
    "\n",
    "def process_query(embeddings):\n",
    "    doc_directory = \"../../../data/convictions/transcripts/iterative\"\n",
    "    iteration_times = 6\n",
    "\n",
    "    for file_name in os.listdir(doc_directory):\n",
    "        if file_name.endswith(\".docx\"):\n",
    "            csv_output_path = os.path.join(doc_directory, f\"{file_name}.csv\")\n",
    "            if os.path.exists(csv_output_path):\n",
    "                logger.info(f\"CSV output for {file_name} already exists. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(doc_directory, file_name)\n",
    "            output_data = []\n",
    "\n",
    "            for iteration in range(1, iteration_times + 1):  \n",
    "                db = preprocess_single_document(file_path, embeddings)\n",
    "                for query in QUERIES:\n",
    "                    officer_data, _ = get_response_from_query(db, query, TEMPERATURE, k)\n",
    "                    for item in officer_data:\n",
    "                        item[\"Query\"] = query\n",
    "                        item[\"Prompt Template for Hyde\"] = PROMPT_TEMPLATE_HYDE\n",
    "                        item[\"Prompt Template for Model\"] = PROMPT_TEMPLATE_MODEL\n",
    "                        item[\"Chunk Size\"] = CHUNK_SIZE\n",
    "                        item[\"Chunk Overlap\"] = CHUNK_OVERLAP\n",
    "                        item[\"Temperature\"] = TEMPERATURE\n",
    "                        item[\"k\"] = k\n",
    "                        item[\"hyde\"] = \"0\"\n",
    "                        item[\"iteration\"] = iteration  \n",
    "                    output_data.extend(officer_data)\n",
    "\n",
    "            output_df = pd.DataFrame(output_data)\n",
    "            output_df.to_csv(csv_output_path, index=False)\n",
    "\n",
    "def main():\n",
    "    embeddings = generate_hypothetical_embeddings()\n",
    "    process_query(embeddings)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
